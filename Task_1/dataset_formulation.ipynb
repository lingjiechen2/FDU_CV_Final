{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to /root/Fudan-CV-final/CIFAR-100/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169001437/169001437 [02:35<00:00, 1088764.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /root/Fudan-CV-final/CIFAR-100/cifar-100-python.tar.gz to /root/Fudan-CV-final/CIFAR-100\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the directory where you want to download the dataset\n",
    "dataset_dir = '/root/Fudan-CV-final/CIFAR-100'\n",
    "\n",
    "# Define the transformations to apply to the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "])\n",
    "\n",
    "# Download the training dataset\n",
    "train_dataset = torchvision.datasets.CIFAR100(\n",
    "    root=dataset_dir,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    # transform=transform\n",
    ")\n",
    "\n",
    "# Download the test dataset\n",
    "test_dataset = torchvision.datasets.CIFAR100(\n",
    "    root=dataset_dir,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    # transform=transform\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the directory where you want to download the dataset\n",
    "dataset_dir = '/root/Fudan-CV-final/CIFAR-100'\n",
    "cifar100_dir = '/root/Fudan-CV-final/CIFAR-100-ImageFolder'\n",
    "\n",
    "# Download the training dataset without transformations\n",
    "train_dataset = torchvision.datasets.CIFAR100(\n",
    "    root=dataset_dir,\n",
    "    train=True,\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "# Function to convert CIFAR-100 dataset to ImageFolder structure\n",
    "def convert_cifar100_to_imagefolder(cifar_dataset, output_dir):\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "    classes = cifar_dataset.classes\n",
    "    for i, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(output_dir, class_name)\n",
    "        os.makedirs(class_dir, exist_ok=True)\n",
    "    \n",
    "    for idx, (image, label) in enumerate(cifar_dataset):\n",
    "        class_name = classes[label]\n",
    "        class_dir = os.path.join(output_dir, class_name)\n",
    "        img_path = os.path.join(class_dir, f'{idx:05d}.png')\n",
    "        image.save(img_path)\n",
    "\n",
    "# Convert the training dataset\n",
    "convert_cifar100_to_imagefolder(train_dataset, os.path.join(cifar100_dir, 'train'))\n",
    "\n",
    "# Download and convert the test dataset without transformations\n",
    "test_dataset = torchvision.datasets.CIFAR100(\n",
    "    root=dataset_dir,\n",
    "    train=False,\n",
    "    download=True,\n",
    ")\n",
    "convert_cifar100_to_imagefolder(test_dataset, os.path.join(cifar100_dir, 'val'))\n",
    "\n",
    "# Now you can use ImageFolder to load the data with the proper transformations during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision.datasets as datasets\n",
    "import sys\n",
    "sys.path.append('/root/Fudan-CV-final/1.MOCO')\n",
    "import moco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Fudan-CV-final/CIFAR-100/cifar-100-python/train\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'moco' has no attribute 'loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m traindir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/root/Fudan-CV-final/CIFAR-100/cifar-100-python\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(traindir)\n\u001b[1;32m      5\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(\n\u001b[0;32m----> 6\u001b[0m         traindir, \u001b[43mmoco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[38;5;241m.\u001b[39mTwoCropsTransform(transforms\u001b[38;5;241m.\u001b[39mCompose(augmentation))\n\u001b[1;32m      7\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'moco' has no attribute 'loader'"
     ]
    }
   ],
   "source": [
    "traindir = os.path.join(\"/root/Fudan-CV-final/CIFAR-100/cifar-100-python\", \"train\")\n",
    "print(traindir)\n",
    "\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "        traindir, moco.loader.TwoCropsTransform(transforms.Compose(augmentation))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import torch# New weights with accuracy 80.858%\n",
    "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "torch.save(model.state_dict(), '/root/Fudan-CV-final/1.MOCO/resnet_18_pretrained.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "import os\n",
    "\n",
    "os.environ['http_proxy'] = 'http://10.176.52.116:7890'\n",
    "os.environ['https_proxy'] = 'http://10.176.52.116:7890'\n",
    "os.environ['all_proxy'] = 'socks5://10.176.52.116:7891'\n",
    "client = OpenAI(api_key='sk-7ZUnzGNOyiwp1u8A2a79101cA6584dDbA3A2Be7bD0E3279a', base_url='https://chatapi.onechats.top/v1')\n",
    "\n",
    "\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": '''多模态模型旨在整合来自不同来源（如文本、图像、声音）的信息，以提高机器学习任务的性能，Transformer架构因其强大的序列建模能力和并行处理能力，在多模态任务重展现出巨大潜力。设想一个智能家居场景，需要开发一个系统，该系统能理解用户的语音指令，并结合环境图像（如房间布局、物品位置）做出准确相应，比如根据用户“请把那本书放在桌子上”的指令，正确识别“那本书”和“桌子”的位置关系，进而控制机器人执行任务。请你设计、开发、评估几个问题讨论，如何开发这个系统？'''},\n",
    "  ],\n",
    "    model=\"gpt-4o\",\n",
    ")\n",
    "\n",
    "message = completion.choices[0].message\n",
    "content = unicodedata.normalize('NFKC', message.content)\n",
    "\n",
    "# print(content)display(Markdown(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "开发一个智能家居系统,该系统能够理解用户的语音指令并结合环境图像做出准确响应,可以从以下几个方面进行设计、开发和评估:\n",
       "\n",
       "### 一、系统设计与架构\n",
       "\n",
       "1. **输入与前处理**\n",
       "   - **语音指令处理**:\n",
       "     - 语音采集与降噪。\n",
       "     - 语音控制识别(ASR),将语音转换为文本。\n",
       "   - **视觉输入处理**:\n",
       "     - 图像采集与预处理(分辨率调整、去噪等)。\n",
       "     - 物体检测与识别(使用预训练模型如YOLO、Faster R-CNN等)。\n",
       "\n",
       "2. **多模态信息融合机制**\n",
       "   - 使用Transformer架构能高效处理序列数据,并将不同模态的信息进行融合。\n",
       "   - 多模态模型(例如,联合训练的多模态版本的BERT或ViT)。\n",
       "\n",
       "3. **上下文理解与任务解析**\n",
       "   - 自然语言理解(NLU),对文本指令进行语义解析。\n",
       "   - 空间关系理解,解析物体位置和关系(比如“书”和“桌子”的空间关系)。\n",
       "\n",
       "4. **任务规划与执行**\n",
       "   - 机器人控制模块,根据解析的指令计算行动路径。\n",
       "   - 动作执行模块,调用机器人的控制接口完成指定任务。\n",
       "\n",
       "### 二、开发步骤\n",
       "\n",
       "1. **数据收集与标注**\n",
       "   - 收集大量智能家居场景的图像数据、语音指令,并进行标注,建立数据集。\n",
       "\n",
       "2. **模型训练**\n",
       "   - 预训练单模态模型:ASR模型、物体检测模型。\n",
       "   - 联合训练多模态模型:使用Transformer架构,整合文本和图像信息。\n",
       "\n",
       "3. **系统集成**\n",
       "   - 将语音处理、图像处理、多模态融合模块集成到一个系统中。\n",
       "\n",
       "4. **功能测试**\n",
       "   - 在模拟环境中测试系统响应的准确性与实效性。\n",
       "   - 不同场景和指令下的鲁棒性测试。\n",
       "\n",
       "### 三、评估标准\n",
       "\n",
       "1. **准确性**:\n",
       "   - **指令识别准确性**:系统能准确识别语音指令的能力。\n",
       "   - **物体识别与关系理解准确性**:正确识别场景中涉及的物体及其关系。\n",
       "\n",
       "2. **响应时间**:\n",
       "   - 系统从接收指令到开始执行任务的总时间。\n",
       "\n",
       "3. **鲁棒性**:\n",
       "   - 系统在不同环境、不同指令下的稳定性和处理能力。\n",
       "   - 对意外情况的处理能力(如语音噪声、图像光线变化等)。\n",
       "\n",
       "4. **用户体验**:\n",
       "   - 用户对系统响应的满意度(通过调查问卷等方式评估)。\n",
       "   - 系统是否易于操作和理解。\n",
       "\n",
       "### 四、面临的挑战与解决方案\n",
       "\n",
       "1. **语音与图像数据的同步处理**:\n",
       "   - 确保语音指令和图像数据能够同步传输和处理,需要高效的数据采集与处理链路。\n",
       "\n",
       "2. **多模态信息融合的有效性**:\n",
       "   - 高效利用Transformer架构进行多模态信息融合,建议探索自注意力机制。\n",
       "\n",
       "3. **复杂场景与稀疏样本的泛化性**:\n",
       "   - 在多样化和复杂的智能家居场景中保证系统能有效工作,可能需要扩展数据集和增强模型的泛化能力。\n",
       "\n",
       "### 五、未来工作\n",
       "\n",
       "1. **持续改进多模态模型的性能**。\n",
       "2. **在实际应用中的迭代与反馈**:\n",
       "   - 根据用户反馈不断改进系统,增加新的功能。\n",
       "3. **扩展系统应用场景**:\n",
       "   - 从智能家居扩展至更多领域,如智能办公、公共场所等。\n",
       "\n",
       "通过以上设计、开发和评估步骤,可以开发出一个能够理解用户语音指令并结合环境图像准确响应的智能家居系统。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to use CIFAR 100 and train ResNet18 model\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "def train_resnet18_on_cifar100():\n",
    "    # Define the directory where you want to download the dataset\n",
    "    dataset_dir = '/root/Fudan-CV-final/CIFAR-100'\n",
    "    cifar100_dir = '/root/Fudan-CV-final/CIFAR-100-ImageFolder'\n",
    "\n",
    "    # Download the training dataset without transformations\n",
    "    train_dataset = torchvision.datasets.CIFAR100(\n",
    "        root=dataset_dir,\n",
    "        train=True,\n",
    "        download=True,\n",
    "    )\n",
    "\n",
    "    # Function to convert CIFAR-100 dataset to ImageFolder structure\n",
    "    def convert_cifar100_to_imagefolder(cifar_dataset, output_dir):\n",
    "        if os.path.exists(output_dir):\n",
    "            shutil.rmtree(output_dir)\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "        classes = cifar_dataset.classes\n",
    "        for i, class_name in enumerate(classes):\n",
    "            class_dir = os.path.join(output_dir, class_name)\n",
    "            os.makedirs(class_dir, exist_ok=True)\n",
    "        \n",
    "        for idx, (image, label) in enumerate(cifar_dataset):\n",
    "            class_name = classes[label]\n",
    "            class_dir = os.path.join(output_dir, class_name)\n",
    "            img_path = os.path.join(class_dir, f'{idx:05d}.png')\n",
    "            image.save(img_path)\n",
    "\n",
    "    # Convert the training dataset\n",
    "    convert_cifar100_to_imagefolder(train_dataset, os.path.join(cifar100_dir, 'train'))\n",
    "\n",
    "    # Download and convert the test dataset without transformations\n",
    "    test_dataset = torchvision.datasets.CIFAR100(\n",
    "        root=dataset_dir,\n",
    "        train=False,\n",
    "        download=True,\n",
    "    )\n",
    "    convert_cifar100_to_imagefolder(test_dataset, os.path.join(cifar100_dir, 'val'))\n",
    "\n",
    "    # Now you can use ImageFolder to load the data with the proper transformations during training\n",
    "\n",
    "    import moco\n",
    "    traindir = os.path.join(\"/root/Fudan-CV-final/CIFAR-100/cifar-100-python\", \"train\")\n",
    "    train_dataset = datasets.ImageFolder(\n",
    "            traindir, moco.loader.TwoCropsTransform(transforms.Compose(augmentation))\n",
    "        )\n",
    "\n",
    "    from torchvision.models import resnet18, ResNet18_Weights\n",
    "    import torch\n",
    "    model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "    torch.save(model.state_dict(), '/root/Fudan-CV-final/1.MOCO/resnet_18_pretrained.pth')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['conv1.weight', 'bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'bn1.num_batches_tracked', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.bn1.bias', 'layer1.0.bn1.running_mean', 'layer1.0.bn1.running_var', 'layer1.0.bn1.num_batches_tracked', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn2.bias', 'layer1.0.bn2.running_mean', 'layer1.0.bn2.running_var', 'layer1.0.bn2.num_batches_tracked', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn1.bias', 'layer1.1.bn1.running_mean', 'layer1.1.bn1.running_var', 'layer1.1.bn1.num_batches_tracked', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.bn2.bias', 'layer1.1.bn2.running_mean', 'layer1.1.bn2.running_var', 'layer1.1.bn2.num_batches_tracked', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn1.bias', 'layer2.0.bn1.running_mean', 'layer2.0.bn1.running_var', 'layer2.0.bn1.num_batches_tracked', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn2.bias', 'layer2.0.bn2.running_mean', 'layer2.0.bn2.running_var', 'layer2.0.bn2.num_batches_tracked', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.0.downsample.1.bias', 'layer2.0.downsample.1.running_mean', 'layer2.0.downsample.1.running_var', 'layer2.0.downsample.1.num_batches_tracked', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn1.bias', 'layer2.1.bn1.running_mean', 'layer2.1.bn1.running_var', 'layer2.1.bn1.num_batches_tracked', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.bn2.bias', 'layer2.1.bn2.running_mean', 'layer2.1.bn2.running_var', 'layer2.1.bn2.num_batches_tracked', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.bn1.bias', 'layer3.0.bn1.running_mean', 'layer3.0.bn1.running_var', 'layer3.0.bn1.num_batches_tracked', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.bn2.bias', 'layer3.0.bn2.running_mean', 'layer3.0.bn2.running_var', 'layer3.0.bn2.num_batches_tracked', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.0.downsample.1.bias', 'layer3.0.downsample.1.running_mean', 'layer3.0.downsample.1.running_var', 'layer3.0.downsample.1.num_batches_tracked', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.bn1.bias', 'layer3.1.bn1.running_mean', 'layer3.1.bn1.running_var', 'layer3.1.bn1.num_batches_tracked', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.bn2.bias', 'layer3.1.bn2.running_mean', 'layer3.1.bn2.running_var', 'layer3.1.bn2.num_batches_tracked', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.bn1.bias', 'layer4.0.bn1.running_mean', 'layer4.0.bn1.running_var', 'layer4.0.bn1.num_batches_tracked', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.bn2.bias', 'layer4.0.bn2.running_mean', 'layer4.0.bn2.running_var', 'layer4.0.bn2.num_batches_tracked', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.0.downsample.1.bias', 'layer4.0.downsample.1.running_mean', 'layer4.0.downsample.1.running_var', 'layer4.0.downsample.1.num_batches_tracked', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight', 'layer4.1.bn1.bias', 'layer4.1.bn1.running_mean', 'layer4.1.bn1.running_var', 'layer4.1.bn1.num_batches_tracked', 'layer4.1.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.bn2.bias', 'layer4.1.bn2.running_mean', 'layer4.1.bn2.running_var', 'layer4.1.bn2.num_batches_tracked', 'fc.weight', 'fc.bias'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"/root/Fudan-CV-final/1.MOCO/resnet_18_pretrained.pth\", map_location=\"cpu\")\n",
    "checkpoint.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
